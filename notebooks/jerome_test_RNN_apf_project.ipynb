{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qAlSZB7RAvX"
      },
      "source": [
        "# Imports and Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OXeNNT3CEtjO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-09 13:08:17.520719: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-09 13:08:17.619626: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-09 13:08:17.622630: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-09 13:08:19.564157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from colorama import Fore, Style\n",
        "from tensorflow import keras\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from colorama import Fore, Style\n",
        "from dateutil.parser import parse\n",
        "from typing import Dict, List, Tuple, Sequence\n",
        "from datetime import datetime\n",
        "\n",
        "from power.params import *\n",
        "from power.ml_ops.data import get_data_with_cache, load_data_to_bq, clean_pv_data\n",
        "from power.ml_ops.model import initialize_model, compile_model, train_model\n",
        "from power.ml_ops.registry import load_model, save_model, save_results\n",
        "from power.ml_ops.cross_val import get_Xi_yi, get_X_y_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa11TRvzRF9A"
      },
      "source": [
        "# Package Functions as reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfGdALOiFljf"
      },
      "outputs": [],
      "source": [
        "def preprocess(min_date:str = '2009-01-01', max_date:str = '2015-01-01') -> None:\n",
        "    \"\"\"\n",
        "    - Query the raw dataset from Le Wagon's BigQuery dataset\n",
        "    - Cache query result as a local CSV if it doesn't exist locally\n",
        "    - Process query data\n",
        "    - Store processed data on your personal BQ (truncate existing table if it exists)\n",
        "    - No need to cache processed data as CSV (it will be cached when queried back from BQ during training)\n",
        "    \"\"\"\n",
        "\n",
        "    print(Fore.MAGENTA + \"\\n ⭐️ Use case: preprocess\" + Style.RESET_ALL)\n",
        "\n",
        "    # Query raw data from BUCKET BigQuery using `get_data_with_cache`\n",
        "    query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM {GCP_PROJECT}.{BQ_DATASET}.raw_pv\n",
        "        ORDER BY _0\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve data using `get_data_with_cache`\n",
        "    data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"raw_pv.csv\")\n",
        "    data_query = get_data_with_cache(\n",
        "        query=query,\n",
        "        gcp_project=GCP_PROJECT,\n",
        "        cache_path=data_query_cache_path,\n",
        "        data_has_header=True\n",
        "    )\n",
        "\n",
        "    # Process data\n",
        "    data_clean = clean_pv_data(data_query)\n",
        "\n",
        "\n",
        "    load_data_to_bq(\n",
        "        data_clean,\n",
        "        gcp_project=GCP_PROJECT,\n",
        "        bq_dataset=BQ_DATASET,\n",
        "        table=f'processed_pv',\n",
        "        truncate=True\n",
        "    )\n",
        "\n",
        "    print(\"✅ preprocess() done \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTJNnbpCROmz"
      },
      "source": [
        "# Load Processed data and split into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5CsTRK4GzQv",
        "outputId": "5b94b49f-2d5e-4a03-bb86-4f09face50fc"
      },
      "outputs": [],
      "source": [
        "print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
        "print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
        "\n",
        "\n",
        "# Load processed data using `get_data_with_cache` in chronological order\n",
        "\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
        "    ORDER BY utc_time\n",
        "\"\"\"\n",
        "\n",
        "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
        "data_processed = get_data_with_cache(\n",
        "    gcp_project=GCP_PROJECT,\n",
        "    query=query,\n",
        "    cache_path=data_processed_cache_path,\n",
        "    data_has_header=True\n",
        ")\n",
        "\n",
        "# the model uses power as feature -> fix that in raw data\n",
        "data_processed = data_processed.rename(columns={'electricity': 'power'})\n",
        "# the processed data form bq needs to be converted to datetime object\n",
        "data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
        "\n",
        "if data_processed.shape[0] < 240:\n",
        "    print(\"❌ Not enough processed data retrieved to train on\")\n",
        "    # return None\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train = data_processed[data_processed['utc_time'] < '2020-01-01']\n",
        "test = data_processed[data_processed['utc_time'] >= '2020-01-01']\n",
        "\n",
        "train = train[['power']]\n",
        "test = test[['power']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOxQQjEGM_wv"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = get_X_y_seq(train,\n",
        "                                number_of_sequences=10_000,\n",
        "                                input_length=48,\n",
        "                                output_length=24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIV2SlY9Rasb"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38EPazt5LcyS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import models, layers, optimizers, metrics\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def initialize_model(X_train, y_train, n_unit=24):\n",
        "\n",
        "    # 1 - RNN architecture\n",
        "    # ======================\n",
        "    model = models.Sequential()\n",
        "\n",
        "    ## 1.1 - Recurrent Layer\n",
        "    model.add(layers.LSTM(n_unit,\n",
        "                          activation='tanh',\n",
        "                          return_sequences = False,\n",
        "                          input_shape=(X_train.shape[1],X_train.shape[2])\n",
        "                          ))\n",
        "    ## 1.2 - Predictive Dense Layers\n",
        "    output_length = y_train.shape[1]\n",
        "    model.add(layers.Dense(output_length, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def compile_model(model, learning_rate=0.02):\n",
        "\n",
        "    # def r_squared(y_true, y_pred):\n",
        "    #     ss_res = K.sum(K.square(y_true - y_pred))\n",
        "    #     ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    #     return (1 - ss_res/(ss_tot + K.epsilon()))\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=adam, metrics=['mae']) #, r_squared])\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model,\n",
        "                X_train,\n",
        "                y_train,\n",
        "                validation_split = 0.3,\n",
        "                batch_size = 32,\n",
        "                epochs = 50):\n",
        "    es = EarlyStopping(monitor = \"val_mae\",\n",
        "                       mode = \"min\",\n",
        "                       patience = 5,\n",
        "                       restore_best_weights = True)\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_split=validation_split,\n",
        "                        shuffle=False,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        callbacks = [es],\n",
        "                        verbose = 0)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwAYdGD-RfTb"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc20G9SnNIqT"
      },
      "outputs": [],
      "source": [
        "model = initialize_model(X_train, y_train, n_unit=24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV6aQ_4hNbhP",
        "outputId": "be4c6c3f-1d62-4de4-e7be-8d3a48bbe346"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odHe3B4jNhC5"
      },
      "outputs": [],
      "source": [
        "model = compile_model(model, learning_rate=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JifCWO3CNpbh"
      },
      "outputs": [],
      "source": [
        "model, history = train_model(model,\n",
        "                                X_train,\n",
        "                                y_train,\n",
        "                                validation_split = 0.3,\n",
        "                                batch_size = 32,\n",
        "                                epochs = 50\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igNpsFd5NuJF",
        "outputId": "788060b8-5a0f-4206-d506-5f428ba9e193"
      },
      "outputs": [],
      "source": [
        "val_mae = np.min(history.history['val_mae'])\n",
        "val_mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJa1p0VwNwPm",
        "outputId": "b4143b8a-f108-4e76-acee-6cff05721de9"
      },
      "outputs": [],
      "source": [
        "params = dict(\n",
        "    context=\"train\",\n",
        "    training_set_size='40 years worth of data',\n",
        "    row_count=len(X_train),\n",
        ")\n",
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9G9Po6uQxuB"
      },
      "source": [
        "# Attempt to save & load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GZiycKoHQl8C",
        "outputId": "ce687816-a31b-428c-c910-48be0f809ec4"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from colorama import Fore, Style\n",
        "from tensorflow import keras\n",
        "from google.cloud import storage\n",
        "\n",
        "from power.params import *\n",
        "\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5reUQJwTRsag",
        "outputId": "15dee431-e4c8-4cb6-d171-8a17ea7084dd"
      },
      "outputs": [],
      "source": [
        "# Save model locally\n",
        "model_path = os.path.join(LOCAL_REGISTRY_PATH, \"models\", f\"{timestamp}.h5\")\n",
        "model.save(model_path)\n",
        "\n",
        "print(\"✅ Model saved locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGybSQQ6SQZG",
        "outputId": "5b98cd8b-a4bd-4648-82c2-21c28a33642f"
      },
      "outputs": [],
      "source": [
        "print(Fore.BLUE + f\"\\nLoad latest model from local registry...\" + Style.RESET_ALL)\n",
        "\n",
        "# Get the latest model version name by the timestamp on disk\n",
        "local_model_directory = os.path.join(LOCAL_REGISTRY_PATH, \"models\")\n",
        "local_model_paths = glob.glob(f\"{local_model_directory}/*\")\n",
        "\n",
        "if not local_model_paths:\n",
        "    print('None')\n",
        "\n",
        "most_recent_model_path_on_disk = sorted(local_model_paths)[-1]\n",
        "\n",
        "print(Fore.BLUE + f\"\\nLoad latest model from disk...\" + Style.RESET_ALL)\n",
        "\n",
        "latest_model = keras.models.load_model(most_recent_model_path_on_disk)\n",
        "\n",
        "print(\"✅ Model loaded from local disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPKxa3OCSkqf",
        "outputId": "5b90266e-ff7b-4133-8a08-4e9b4a896f0f"
      },
      "outputs": [],
      "source": [
        "latest_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Pred function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pred(X_pred:str = '2013-05-08 12:00:00') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Make a prediction using the latest trained model\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n⭐️ Use case: predict\")\n",
        "\n",
        "    # X_pred = datetime.strptime(X_pred, '%Y-%m-%d %H:%M:%S')\n",
        "    # reference_datetime = datetime.strptime(\"1980-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')\n",
        "    # time_difference = X_pred - reference_datetime\n",
        "    # time_difference_hours = time_difference.total_seconds() / 3600\n",
        "    # input_date = X_test[time_difference_hours-47: time_difference_hours+1]\n",
        "\n",
        "\n",
        "\n",
        "    # if X_pred is None:\n",
        "    #     X_pred = pd.DataFrame(dict(\n",
        "    #     pickup_datetime=[pd.Timestamp(\"2013-07-06 17:18:00\", tz='UTC')],\n",
        "    #     pickup_longitude=[-73.950655],\n",
        "    #     pickup_latitude=[40.783282],\n",
        "    #     dropoff_longitude=[-73.984365],\n",
        "    #     dropoff_latitude=[40.769802],\n",
        "    #     passenger_count=[1],\n",
        "    # ))\n",
        "\n",
        "    model = load_model()\n",
        "    assert model is not None\n",
        "\n",
        "    # X_processed = preprocess_features(X_pred)\n",
        "    # y_pred = model.predict(X_processed)\n",
        "\n",
        "    # print(\"\\n✅ prediction done: \", y_pred, y_pred.shape, \"\\n\")\n",
        "    print(\"\\n✅ prediction done: \\n\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
