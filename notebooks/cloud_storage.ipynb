{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "# from colorama import Fore, Style\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache, clean_data, load_data_to_bq\n",
    "from power.ml_ops.model import initialize_model, compile_model, train_model, evaluate_model\n",
    "from power.ml_ops.preprocessor import preprocess_features\n",
    "from power.ml_ops.registry import load_model, save_model, save_results\n",
    "from power.ml_ops.registry import mlflow_run, mlflow_transition_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bigquery' from 'google.cloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bigquery\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_with_cache\u001b[39m(\n\u001b[1;32m      4\u001b[0m         gcp_project:\u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m         query:\u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      6\u001b[0m         cache_path:Path,\n\u001b[1;32m      7\u001b[0m         data_has_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Retrieve `query` data from BigQuery, or from `cache_path` if the file exists\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Store at `cache_path` if retrieved from BigQuery for future use\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'bigquery' from 'google.cloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def get_data_with_cache(\n",
    "        gcp_project:str,\n",
    "        query:str,\n",
    "        cache_path:Path,\n",
    "        data_has_header=True\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve `query` data from BigQuery, or from `cache_path` if the file exists\n",
    "    Store at `cache_path` if retrieved from BigQuery for future use\n",
    "    \"\"\"\n",
    "    if cache_path.is_file():\n",
    "        print(Fore.BLUE + \"\\nLoad data from local CSV...\" + Style.RESET_ALL)\n",
    "        df = pd.read_csv(cache_path, header='infer' if data_has_header else None)\n",
    "    else:\n",
    "        print(Fore.BLUE + \"\\nLoad data from BigQuery server...\" + Style.RESET_ALL)\n",
    "        client = bigquery.Client(project=gcp_project)\n",
    "        query_job = client.query(query)\n",
    "        result = query_job.result()\n",
    "        df = result.to_dataframe()\n",
    "\n",
    "        # Store as CSV if the BQ query returned at least one valid line\n",
    "        if df.shape[0] > 1:\n",
    "            df.to_csv(cache_path, header=data_has_header, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Data loaded, with shape {df.shape}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_bq(\n",
    "        data: pd.DataFrame,\n",
    "        gcp_project:str,\n",
    "        bq_dataset:str,\n",
    "        # table: str,\n",
    "        # truncate: bool\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    - Save the DataFrame to google storage\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(data, pd.DataFrame)\n",
    "    full_table_name = f\"{gcp_project}.{bq_dataset}.{table}\"\n",
    "    print(Fore.BLUE + f\"\\nSave data to BigQuery @ {full_table_name}...:\" + Style.RESET_ALL)\n",
    "\n",
    "    # Load data onto full_table_name\n",
    "\n",
    "    # üéØ HINT for \"*** TypeError: expected bytes, int found\":\n",
    "    # After preprocessing the data, your original column names are gone (print it to check),\n",
    "    # so ensure that your column names are *strings* that start with either\n",
    "    # a *letter* or an *underscore*, as BQ does not accept anything else\n",
    "\n",
    "    # TODO: simplify this solution if possible, but students may very well choose another way to do it\n",
    "    # We don't test directly against their own BQ tables, but only the result of their query\n",
    "\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define write mode and schema\n",
    "    write_mode = \"WRITE_TRUNCATE\" if truncate else \"WRITE_APPEND\"\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=write_mode)\n",
    "\n",
    "    print(f\"\\n{'Write' if truncate else 'Append'} {full_table_name} ({data.shape[0]} rows)\")\n",
    "\n",
    "    # Load data\n",
    "    job = client.load_table_from_dataframe(data, full_table_name, job_config=job_config)\n",
    "    result = job.result()  # wait for the job to complete\n",
    "\n",
    "    print(f\"‚úÖ Data saved to bigquery, with shape {data.shape}\")\n",
    "\n",
    "    ##### NEW CODE #####\n",
    "    model_path = os.path.join(LOCAL_REGISTRY_PATH, \"models\", f\"{timestamp}.h5\")\n",
    "    model_filename = model_path.split(\"/\")[-1] # e.g. \"20230208-161047.h5\" for instance\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(f\"models/{model_filename}\")\n",
    "    blob.upload_from_filename(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: keras.Model = None) -> None:\n",
    "    \"\"\"\n",
    "    Persist trained model locally on the hard drive at f\"{LOCAL_REGISTRY_PATH}/models/{timestamp}.h5\"\n",
    "    - if MODEL_TARGET='gcs', also persist it in your bucket on GCS at \"models/{timestamp}.h5\" --> unit 02 only\n",
    "    - if MODEL_TARGET='mlflow', also persist it on MLflow instead of GCS (for unit 0703 only) --> unit 03 only\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Save model locally\n",
    "    model_path = os.path.join(LOCAL_REGISTRY_PATH, \"models\", f\"{timestamp}.h5\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    print(\"‚úÖ Model saved locally\")\n",
    "\n",
    "    if MODEL_TARGET == \"gcs\":\n",
    "        # üéÅ We give you this piece of code as a gift. Please read it carefully! Add a breakpoint if needed!\n",
    "\n",
    "        model_filename = model_path.split(\"/\")[-1] # e.g. \"20230208-161047.h5\" for instance\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(BUCKET_NAME)\n",
    "        blob = bucket.blob(f\"models/{model_filename}\")\n",
    "        blob.upload_from_filename(model_path)\n",
    "\n",
    "        print(\"‚úÖ Model saved to GCS\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    if MODEL_TARGET == \"mlflow\":\n",
    "        mlflow.tensorflow.log_model(\n",
    "            model=model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=MLFLOW_MODEL_NAME\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Model saved to MLflow\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Raw Dataset & Store it on personal Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(min_date:str = '2009-01-01', max_date:str = '2015-01-01') -> None:\n",
    "    \"\"\"\n",
    "    - Query the raw dataset from Le Wagon's BigQuery dataset\n",
    "    - Cache query result as a local CSV if it doesn't exist locally\n",
    "    - Process query data\n",
    "    - Store processed data on your personal BQ (truncate existing table if it exists)\n",
    "    - No need to cache processed data as CSV (it will be cached when queried back from BQ during training)\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n ‚≠êÔ∏è Use case: preprocess\" + Style.RESET_ALL)\n",
    "\n",
    "    # Query raw data from BUCKET BigQuery using `get_data_with_cache`\n",
    "    min_date = parse(min_date).strftime('%Y-%m-%d') # e.g '2009-01-01'\n",
    "    max_date = parse(max_date).strftime('%Y-%m-%d') # e.g '2009-01-01'\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT {\",\".join(COLUMN_NAMES_RAW)}\n",
    "        FROM {GCP_PROJECT_JEROME}.{BQ_DATASET}.raw_{BQ_DATASET}\n",
    "        WHERE pickup_datetime BETWEEN '{min_date}' AND '{max_date}'\n",
    "        ORDER BY pickup_datetime\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve data using `get_data_with_cache`\n",
    "    data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"query_{min_date}_{max_date}_{DATA_SIZE}.csv\")\n",
    "    data_query = get_data_with_cache(\n",
    "        query=query,\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        cache_path=data_query_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "    # Process data\n",
    "\n",
    "    ##### MARIUS #####\n",
    "\n",
    "\n",
    "    load_data_to_bq(\n",
    "        data_processed_with_timestamp,\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        bq_dataset=BQ_DATASET,\n",
    "        table=f'processed_{DATA_SIZE}',\n",
    "        truncate=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ preprocess() done \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
