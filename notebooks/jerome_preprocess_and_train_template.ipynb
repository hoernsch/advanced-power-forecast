{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from google.cloud import bigquery, storage\n",
    "\n",
    "from pathlib import Path\n",
    "from colorama import Fore, Style\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from power.ml_ops.data import get_pv_data, clean_pv_data\n",
    "\n",
    "from power.params import *\n",
    "# from power.ml_ops.data import get_data_with_cache, clean_data, load_data_to_bq\n",
    "# from power.ml_ops.model import initialize_model, compile_model, train_model, evaluate_model\n",
    "# from power.ml_ops.preprocessor import preprocess_features\n",
    "# from power.ml_ops.registry import load_model, save_model, save_results\n",
    "# from power.ml_ops.registry import mlflow_run, mlflow_transition_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data_with_cache(\n",
    "        gcp_project:str,\n",
    "        query:str,\n",
    "        cache_path:Path,\n",
    "        data_has_header=True\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve `query` data from BigQuery, or from `cache_path` if the file exists\n",
    "    Store at `cache_path` if retrieved from BigQuery for future use\n",
    "    \"\"\"\n",
    "    if cache_path.is_file():\n",
    "        print(Fore.BLUE + \"\\nLoad data from local CSV...\" + Style.RESET_ALL)\n",
    "        df = pd.read_csv(cache_path, header='infer' if data_has_header else None)\n",
    "    else:\n",
    "        print(Fore.BLUE + \"\\nLoad data from BigQuery server...\" + Style.RESET_ALL)\n",
    "        client = bigquery.Client(project=gcp_project)\n",
    "        query_job = client.query(query)\n",
    "        result = query_job.result()\n",
    "        df = result.to_dataframe()\n",
    "\n",
    "        # Store as CSV if the BQ query returned at least one valid line\n",
    "        if df.shape[0] > 1:\n",
    "            df.to_csv(cache_path, header=data_has_header, index=False)\n",
    "\n",
    "    print(f\"✅ Data loaded, with shape {df.shape}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      " ⭐️ Use case: preprocess\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GCP_PROJECT_JEROME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(Fore\u001b[38;5;241m.\u001b[39mMAGENTA \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ⭐️ Use case: preprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m Style\u001b[38;5;241m.\u001b[39mRESET_ALL)\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SELECT *\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;124m    FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mGCP_PROJECT_JEROME\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBQ_DATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.raw_pv\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    ORDER BY _0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m query\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GCP_PROJECT_JEROME' is not defined"
     ]
    }
   ],
   "source": [
    "print(Fore.MAGENTA + \"\\n ⭐️ Use case: preprocess\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT_JEROME}.{BQ_DATASET}.raw_pv\n",
    "    ORDER BY _0\n",
    "\"\"\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data using `get_data_with_cache`\n",
    "data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"raw_pv.csv\")\n",
    "data_query = get_data_with_cache(\n",
    "    query=query,\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    cache_path=data_query_cache_path,\n",
    "    data_has_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_bq(\n",
    "        data: pd.DataFrame,\n",
    "        gcp_project:str,\n",
    "        bq_dataset:str,\n",
    "        table: str,\n",
    "        truncate: bool\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    - Save the DataFrame to BigQuery\n",
    "    - Empty the table beforehand if `truncate` is True, append otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(data, pd.DataFrame)\n",
    "    full_table_name = f\"{gcp_project}.{bq_dataset}.{table}\"\n",
    "    print(Fore.BLUE + f\"\\nSave data to BigQuery @ {full_table_name}...:\" + Style.RESET_ALL)\n",
    "\n",
    "    # Load data onto full_table_name\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define write mode and schema\n",
    "    write_mode = \"WRITE_TRUNCATE\" if truncate else \"WRITE_APPEND\"\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=write_mode)\n",
    "\n",
    "    print(f\"\\n{'Write' if truncate else 'Append'} {full_table_name} ({data.shape[0]} rows)\")\n",
    "\n",
    "    # Load data\n",
    "    job = client.load_table_from_dataframe(data, full_table_name, job_config=job_config)\n",
    "    result = job.result()  # wait for the job to complete\n",
    "\n",
    "    print(f\"✅ Data saved to bigquery, with shape {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_query.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_pv_data(data_query)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_to_bq(\n",
    "        clean_data,\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        bq_dataset=BQ_DATASET,\n",
    "        table=f'processed_pv',\n",
    "        truncate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test whole preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(min_date:str = '2009-01-01', max_date:str = '2015-01-01') -> None:\n",
    "    \"\"\"\n",
    "    - Query the raw dataset from Le Wagon's BigQuery dataset\n",
    "    - Cache query result as a local CSV if it doesn't exist locally\n",
    "    - Process query data\n",
    "    - Store processed data on your personal BQ (truncate existing table if it exists)\n",
    "    - No need to cache processed data as CSV (it will be cached when queried back from BQ during training)\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n ⭐️ Use case: preprocess\" + Style.RESET_ALL)\n",
    "\n",
    "    # Query raw data from BUCKET BigQuery using `get_data_with_cache`\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT_WAGON}.{BQ_DATASET}.raw_pv\n",
    "        ORDER BY _0\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve data using `get_data_with_cache`\n",
    "    data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"raw_pv.csv\")\n",
    "    data_query = get_data_with_cache(\n",
    "        query=query,\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        cache_path=data_query_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "    # Process data\n",
    "    data_clean = clean_data(data_query)\n",
    "\n",
    "\n",
    "    load_data_to_bq(\n",
    "        data_clean,\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        bq_dataset=BQ_DATASET,\n",
    "        table=f'processed_pv',\n",
    "        truncate=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ preprocess() done \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test whole training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        min_date:str = '2009-01-01',\n",
    "        max_date:str = '2015-01-01',\n",
    "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
    "        learning_rate=0.0005,\n",
    "        batch_size = 256,\n",
    "        patience = 2\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    - Download processed data from your BQ table (or from cache if it exists)\n",
    "    - Train on the preprocessed dataset (which should be ordered by date)\n",
    "    - Store training results and model weights\n",
    "\n",
    "    Return val_mae as a float\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "    # Load processed data using `get_data_with_cache` in chronological order\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query,\n",
    "        cache_path=data_processed_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "    if data_processed.shape[0] < 10:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "    # Create (X_train_processed, y_train, X_val_processed, y_val)\n",
    "    # < MARIUS - ALI CODE HERE >\n",
    "\n",
    "    # Train model using `model.py`\n",
    "    model = load_model()\n",
    "\n",
    "    if model is None:\n",
    "        model = initialize_model(input_shape=X_train_processed.shape[1:])\n",
    "\n",
    "    model = compile_model(model, learning_rate=learning_rate)\n",
    "    model, history = train_model(\n",
    "        model, X_train_processed, y_train,\n",
    "        batch_size=batch_size,\n",
    "        patience=patience,\n",
    "        validation_data=(X_val_processed, y_val)\n",
    "    )\n",
    "\n",
    "    val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "    params = dict(\n",
    "        context=\"train\",\n",
    "        training_set_size=DATA_SIZE,\n",
    "        row_count=len(X_train_processed),\n",
    "    )\n",
    "\n",
    "    # Save results on the hard drive using taxifare.ml_logic.registry\n",
    "    save_results(params=params, metrics=dict(mae=val_mae))\n",
    "\n",
    "    # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "    save_model(model=model)\n",
    "\n",
    "    print(\"✅ train() done \\n\")\n",
    "\n",
    "    return val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test whole evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        min_date:str = '2014-01-01',\n",
    "        max_date:str = '2015-01-01',\n",
    "        stage: str = \"Production\"\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the latest production model on processed data\n",
    "    Return MAE as a float\n",
    "    \"\"\"\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: evaluate\" + Style.RESET_ALL)\n",
    "\n",
    "    model = load_model(stage=stage)\n",
    "    assert model is not None\n",
    "\n",
    "\n",
    "    # Query your BigQuery processed table and get data_processed using `get_data_with_cache`\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT_WAGON}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query,\n",
    "        cache_path=data_processed_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "    if data_processed.shape[0] == 0:\n",
    "        print(\"❌ No data to evaluate on\")\n",
    "        return None\n",
    "\n",
    "    data_processed = data_processed.to_numpy()\n",
    "\n",
    "    X_new = data_processed[:, :-1]\n",
    "    y_new = data_processed[:, -1]\n",
    "\n",
    "    metrics_dict = evaluate_model(model=model, X=X_new, y=y_new)\n",
    "    mae = metrics_dict[\"mae\"]\n",
    "\n",
    "    params = dict(\n",
    "        context=\"evaluate\", # Package behavior\n",
    "        training_set_size=DATA_SIZE,\n",
    "        row_count=len(X_new)\n",
    "    )\n",
    "\n",
    "    save_results(params=params, metrics=metrics_dict)\n",
    "\n",
    "    print(\"✅ evaluate() done \\n\")\n",
    "\n",
    "    return mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
