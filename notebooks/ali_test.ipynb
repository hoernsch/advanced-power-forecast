{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 16:23:20.913953: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-08 16:23:21.369791: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-08 16:23:21.374005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 16:23:23.292529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from pathlib import Path\n",
    "# from colorama import Fore, Style\n",
    "# from dateutil.parser import parse\n",
    "\n",
    "# from power.params import *\n",
    "# from power.ml_ops.data import get_data_with_cache, load_data_to_bq, clean_pv_data\n",
    "# from power.ml_ops.model import init_RNN, init_baseline_mean\n",
    "# from power.ml_ops.registry import load_model, save_model, save_results\n",
    "# from power.ml_ops.cross_val import get_X_y_seq\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from colorama import Fore, Style\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache, load_data_to_bq, clean_pv_data\n",
    "from power.ml_ops.model import initialize_model, compile_model, train_model\n",
    "from power.ml_ops.registry import load_model, save_model, save_results\n",
    "from power.ml_ops.cross_val import get_X_y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      " ⭐️ Use case: preprocess\u001b[0m\n",
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (376944, 8)\n",
      "# data cleaned\n",
      "\u001b[34m\n",
      "Save data to BigQuery @ linen-sun-411222.power.processed_pv...:\u001b[0m\n",
      "\n",
      "Write linen-sun-411222.power.processed_pv (376944 rows)\n",
      "✅ Data saved to bigquery, with shape (376944, 3)\n",
      "✅ preprocess() done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Fore.MAGENTA + \"\\n ⭐️ Use case: preprocess\" + Style.RESET_ALL)\n",
    "\n",
    "# Query raw data from BUCKET BigQuery using `get_data_with_cache`\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT}.{BQ_DATASET}.raw_pv\n",
    "    ORDER BY _0\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve data using `get_data_with_cache`\n",
    "data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"raw_pv.csv\")\n",
    "data_query = get_data_with_cache(\n",
    "    query=query,\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    cache_path=data_query_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "# Process data\n",
    "data_clean = clean_pv_data(data_query)\n",
    "\n",
    "\n",
    "load_data_to_bq(\n",
    "    data_clean,\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    bq_dataset=BQ_DATASET,\n",
    "    table=f'processed_pv',\n",
    "    truncate=True\n",
    ")\n",
    "\n",
    "print(\"✅ preprocess() done \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation MAE values\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plotting R-squared for Train and Validation\n",
    "#plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['r_squared'], label='Train R-squared')\n",
    "plt.plot(history.history['val_r_squared'], label='Validation R-squared')\n",
    "plt.title('Model R-squared')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        min_date:str = '2009-01-01',\n",
    "        max_date:str = '2015-01-01',\n",
    "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
    "        learning_rate=0.02,\n",
    "        batch_size = 32,\n",
    "        patience = 5\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    - Download processed data from your BQ table (or from cache if it exists)\n",
    "    - Train on the preprocessed dataset (which should be ordered by date)\n",
    "    - Store training results and model weights\n",
    "\n",
    "    Return val_mae as a float\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "    # Load processed data using `get_data_with_cache` in chronological order\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query,\n",
    "        cache_path=data_processed_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "    data_processed = data_processed.rename(columns={'electricity': 'power'})\n",
    "\n",
    "    data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
    "\n",
    "    if data_processed.shape[0] < 240:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train = data_processed[data_processed['utc_time'] < '2020-01-01']\n",
    "    test = data_processed[data_processed['utc_time'] >= '2020-01-01']\n",
    "\n",
    "    train = train[['power']]\n",
    "    test = test[['power']]\n",
    "\n",
    "    X_train, y_train = get_X_y_seq(train,\n",
    "                                   number_of_sequences=10_000,\n",
    "                                   input_length=48,\n",
    "                                   output_length=24)\n",
    "\n",
    "    X_test, y_test = get_X_y_seq(test,\n",
    "                                 number_of_sequences=1_000,\n",
    "                                 input_length=48,\n",
    "                                 output_length=24)\n",
    "\n",
    "\n",
    "    # Train model using `model.py`\n",
    "    model = load_model()\n",
    "\n",
    "    if model is None:\n",
    "        model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "    model = compile_model(model, learning_rate=learning_rate)\n",
    "    model, history = train_model(model,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                validation_split = 0.3,\n",
    "                                batch_size = 32,\n",
    "                                epochs = 50\n",
    "                                )\n",
    "\n",
    "    val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "    params = dict(\n",
    "        context=\"train\",\n",
    "        training_set_size='40 years worth of data',\n",
    "        row_count=len(X_train),\n",
    "    )\n",
    "\n",
    "    # Save results on the hard drive using taxifare.ml_logic.registry\n",
    "    save_results(params=params, metrics=dict(mae=val_mae))\n",
    "\n",
    "    # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "    save_model(model=model)\n",
    "\n",
    "    print(\"✅ train() done \\n\")\n",
    "\n",
    "    return val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "⭐️ Use case: train\u001b[0m\n",
      "\u001b[34m\n",
      "Loading preprocessed validation data...\u001b[0m\n",
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (376944, 3)\n",
      "\u001b[34m\n",
      "Load latest model from local registry...\u001b[0m\n",
      "\u001b[34m\n",
      "Load latest model from disk...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 16:24:12.855363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 16:24:12.855970: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-03-08 16:24:13.263772: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-03-08 16:24:13.267341: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-03-08 16:24:13.269070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown metric function: 'r_squared'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_mae \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(min_date, max_date, split_ratio, learning_rate, batch_size, patience)\u001b[0m\n\u001b[1;32m     59\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m get_X_y_seq(test,\n\u001b[1;32m     60\u001b[0m                              number_of_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m,\n\u001b[1;32m     61\u001b[0m                              input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m,\n\u001b[1;32m     62\u001b[0m                              output_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Train model using `model.py`\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     model \u001b[38;5;241m=\u001b[39m initialize_model(X_train, y_train, n_unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "File \u001b[0;32m~/code/Alee7hub/Final_Project/advanced-power-forecast/power/ml_ops/registry.py:89\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(stage)\u001b[0m\n\u001b[1;32m     85\u001b[0m most_recent_model_path_on_disk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(local_model_paths)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(Fore\u001b[38;5;241m.\u001b[39mBLUE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoad latest model from disk...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m Style\u001b[38;5;241m.\u001b[39mRESET_ALL)\n\u001b[0;32m---> 89\u001b[0m latest_model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmost_recent_model_path_on_disk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded from local disk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latest_model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/power/lib/python3.10/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/power/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/power/lib/python3.10/site-packages/keras/saving/legacy/serialization.py:543\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    541\u001b[0m     obj \u001b[38;5;241m=\u001b[39m module_objects\u001b[38;5;241m.\u001b[39mget(object_name)\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Classes passed by name are instantiated with no args, functions are\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# returned as-is.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown metric function: 'r_squared'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "val_mae = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '2013-05-08 09:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = datetime.strptime(a, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_datetime = datetime.strptime(\"1980-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the two datetime objects\n",
    "time_difference = a - reference_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_difference_hours = time_difference.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_difference_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test\n",
    "\n",
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pv_data(input_date: str, n_days=10):\n",
    "    pv_data_clean = data_clean\n",
    "    input_timestamp = pd.Timestamp(input_date, tz='UTC')\n",
    "    idx = pv_data_clean[pv_data_clean.utc_time == input_timestamp].index[0]\n",
    "\n",
    "    n_rows = 24 * n_days\n",
    "    if idx <= n_rows:\n",
    "        df = pv_data_clean.iloc[0:idx+24,:]\n",
    "    else:\n",
    "        df = pv_data_clean.iloc[idx-n_rows:idx+24,:].reset_index()\n",
    "    extracted_data = {\n",
    "        'utc_time':df.get('utc_time').tolist(),\n",
    "        'local_time':df.get('local_time').tolist(),\n",
    "        'electricity':df.get('electricity').tolist()\n",
    "    }\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(extract_pv_data('2013-05-08 09:00:00', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "        \"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "data_processed = data_processed.rename(columns={'electricity': 'power'})\n",
    "\n",
    "data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
